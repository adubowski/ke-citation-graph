{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4999it [00:01, 4640.99it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Database wrapper from: https://towardsdatascience.com/create-a-graph-database-in-neo4j-using-python-4172d40f89c4\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "\n",
    "class Neo4jConnection:\n",
    "\n",
    "    def __init__(self, uri, user, pwd):\n",
    "        self.__uri = uri\n",
    "        self.__user = user\n",
    "        self.__pwd = pwd\n",
    "        self.__driver = None\n",
    "        try:\n",
    "            self.__driver = GraphDatabase.driver(self.__uri, auth=(self.__user, self.__pwd))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create the driver:\", e)\n",
    "\n",
    "    def close(self):\n",
    "        if self.__driver is not None:\n",
    "            self.__driver.close()\n",
    "\n",
    "    def query(self, query, parameters=None, db=None):\n",
    "        assert self.__driver is not None, \"Driver not initialized!\"\n",
    "        session = None\n",
    "        response = None\n",
    "        try:\n",
    "            session = self.__driver.session(database=db) if db is not None else self.__driver.session()\n",
    "            response = list(session.run(query, parameters))\n",
    "        except Exception as e:\n",
    "            print(\"Query failed:\", e)\n",
    "        finally:\n",
    "            if session is not None:\n",
    "                session.close()\n",
    "        return response\n",
    "\n",
    "port = 7687 # Check if is the case for your server!\n",
    "\n",
    "conn = Neo4jConnection(uri=\"bolt://localhost:\"+str(port),\n",
    "                       user=\"driver\",\n",
    "                       pwd=\"driver\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conn.query('CREATE CONSTRAINT papers IF NOT EXISTS ON (p:Paper)     ASSERT p.id IS UNIQUE')\n",
    "conn.query('CREATE CONSTRAINT authors IF NOT EXISTS ON (a:Author) ASSERT a.name IS UNIQUE')\n",
    "conn.query('CREATE CONSTRAINT categories IF NOT EXISTS ON (c:Category) ASSERT c.category IS UNIQUE')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conn.query('DROP CONSTRAINT ON (a:Author) ASSERT a.name IS UNIQUE')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def insert_data(query, rows, batch_size = 10000):\n",
    "    # Function to handle the updating the Neo4j database in batch mode.\n",
    "\n",
    "    total = 0\n",
    "    batch = 0\n",
    "    start = time.time()\n",
    "    result = None\n",
    "\n",
    "    while batch * batch_size < len(rows):\n",
    "\n",
    "        res = conn.query(query,\n",
    "                         parameters = {'rows': rows[batch*batch_size:(batch+1)*batch_size].to_dict('records')})\n",
    "        total += res[0]['total']\n",
    "        batch += 1\n",
    "        result = {\"total_inserted\":total,\n",
    "                  \"batches_done\":batch,\n",
    "                  \"total_time\":time.time()-start}\n",
    "        print(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "def add_papers(rows, batch_size=5000):\n",
    "   # Adds paper nodes and (:Author)--(:Paper)\n",
    "   query = '''\n",
    "    // Create papers\n",
    "    UNWIND $rows as paper\n",
    "    MERGE (p:Paper {paperid: paper.id})\n",
    "    ON CREATE SET\n",
    "    p.title = paper.title,\n",
    "    p.year = paper.year,\n",
    "    p.n_citation = paper.n_citation,\n",
    "    p.doi = paper.doi\n",
    "\n",
    "    // Match authors\n",
    "    WITH paper, p\n",
    "    UNWIND  paper.authors AS author\n",
    "    MERGE (a:Author {authorid: author.id})\n",
    "    ON CREATE SET a.name = author.name\n",
    "    MERGE (a)-[:AUTHORED]->(p)\n",
    "\n",
    "    // Match references\n",
    "    WITH paper, p\n",
    "    UNWIND  paper.references AS refid\n",
    "    MATCH (r:Paper {paperid:refid})\n",
    "    MERGE (p)-[:references]->(r)\n",
    "    RETURN count(p:Paper) as total\n",
    "   '''\n",
    "\n",
    "   return insert_data(query, rows, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99999it [00:13, 7199.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 19, 'batches': 1, 'time': 70.04099345207214}\n",
      "{'total': 45, 'batches': 2, 'time': 143.55730032920837}\n",
      "{'total': 55, 'batches': 3, 'time': 213.68727493286133}\n",
      "{'total': 70, 'batches': 4, 'time': 281.48117089271545}\n",
      "{'total': 99, 'batches': 5, 'time': 352.1817169189453}\n",
      "{'total': 123, 'batches': 6, 'time': 444.3081476688385}\n",
      "{'total': 149, 'batches': 7, 'time': 548.1077544689178}\n",
      "{'total': 170, 'batches': 8, 'time': 676.0063235759735}\n",
      "{'total': 196, 'batches': 9, 'time': 818.0243558883667}\n",
      "{'total': 215, 'batches': 10, 'time': 964.2936537265778}\n",
      "{'total': 249, 'batches': 11, 'time': 1124.552337884903}\n",
      "{'total': 310, 'batches': 12, 'time': 1308.9344944953918}\n",
      "{'total': 343, 'batches': 13, 'time': 1481.1513953208923}\n",
      "{'total': 379, 'batches': 14, 'time': 1670.1773149967194}\n",
      "{'total': 436, 'batches': 15, 'time': 1896.545137643814}\n",
      "{'total': 505, 'batches': 16, 'time': 2119.4188482761383}\n",
      "{'total': 547, 'batches': 17, 'time': 2358.314304590225}\n",
      "{'total': 594, 'batches': 18, 'time': 2666.6610946655273}\n",
      "{'total': 664, 'batches': 19, 'time': 2946.9498014450073}\n",
      "{'total': 776, 'batches': 20, 'time': 3240.4559292793274}\n",
      "{'total': 826, 'batches': 21, 'time': 3622.449070930481}\n"
     ]
    }
   ],
   "source": [
    "file = \"data/dblp_papers_v11.txt\"\n",
    "\n",
    "subset = [\"id\", \"title\", \"year\", \"n_citation\", \"doi\", \"authors\", \"references\"]\n",
    "\n",
    "# TODO: Might be possible to speed up inserts by using just the name for matching instead of ID https://stackoverflow.com/a/23609143/9994398\n",
    "with open(file, 'r') as f:\n",
    "    while True:\n",
    "        try:\n",
    "            lines = 100000\n",
    "            rows  = []\n",
    "            for line in tqdm(f):\n",
    "                rows.append(json.loads(line))\n",
    "                lines -= 1\n",
    "                if lines == 0: break\n",
    "            df = pd.DataFrame(rows)\n",
    "            add_papers(df[subset])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_stack_exchange_df():\n",
    "    path = \"data/Posts.xml\"\n",
    "    with open(path, 'r') as f:\n",
    "        try:\n",
    "            lines = 100\n",
    "            raw_data  = []\n",
    "            for line in tqdm(f):\n",
    "                raw_data.append(line)\n",
    "                lines -= 1\n",
    "                if lines == 0: break\n",
    "            return pd.read_xml(raw_data)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "df = get_stack_exchange_df()\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3acfef7138f70d2429ae48cbaca4cc964d0577b5663fbd0bd04cb7bf77da6df6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('maat-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}