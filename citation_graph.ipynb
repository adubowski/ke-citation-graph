{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Database wrapper from: https://towardsdatascience.com/create-a-graph-database-in-neo4j-using-python-4172d40f89c4\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Neo4jConnection:\n",
    "\n",
    "    def __init__(self, uri, user, pwd):\n",
    "        self.__uri = uri\n",
    "        self.__user = user\n",
    "        self.__pwd = pwd\n",
    "        self.__driver = None\n",
    "        try:\n",
    "            self.__driver = GraphDatabase.driver(self.__uri, auth=(self.__user, self.__pwd))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create the driver:\", e)\n",
    "\n",
    "    def close(self):\n",
    "        if self.__driver is not None:\n",
    "            self.__driver.close()\n",
    "\n",
    "    def query(self, query, parameters=None, db=None):\n",
    "        assert self.__driver is not None, \"Driver not initialized!\"\n",
    "        session = None\n",
    "        response = None\n",
    "        try:\n",
    "            session = self.__driver.session(database=db) if db is not None else self.__driver.session()\n",
    "            response = list(session.run(query, parameters))\n",
    "        except Exception as e:\n",
    "            print(\"Query failed:\", e)\n",
    "        finally:\n",
    "            if session is not None:\n",
    "                session.close()\n",
    "        return response\n",
    "\n",
    "port = 7687 # Check if is the case for your server!\n",
    "\n",
    "conn = Neo4jConnection(uri=\"bolt://localhost:\"+str(port),\n",
    "                       user=\"driver\",\n",
    "                       pwd=\"driver\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def insert_data(query, rows, batch_size = 10000):\n",
    "    # Function to handle the updating the Neo4j database in batch mode.\n",
    "\n",
    "    total = 0\n",
    "    batch = 0\n",
    "    start = time.time()\n",
    "    result = None\n",
    "\n",
    "    for batch in tqdm(range(len(rows) // batch_size)):\n",
    "        batch_start = time.time()\n",
    "        res = conn.query(query,\n",
    "                         parameters = {'rows': rows[batch*batch_size:(batch+1)*batch_size].to_dict('records')})\n",
    "        total += res[0]['total']\n",
    "        result = {\"batch_size\": batch_size,\n",
    "                  \"batches_done\": batch,\n",
    "                  \"batch_time\": time.time() - batch_start,\n",
    "                  \"total_time\": time.time()-start}\n",
    "        # print(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "def add_papers(rows, batch_size=5000):\n",
    "   # Adds paper nodes and relationships (:Author)-[:AUTHORED]-(:Paper), (:Paper)-[:REFERENCES]-(:Paper)\n",
    "   query = '''\n",
    "    // Create papers\n",
    "    UNWIND $rows as paper\n",
    "    MERGE (p:Paper {paperid: paper.id})\n",
    "    ON CREATE SET\n",
    "    p.title = paper.title,\n",
    "    p.year = paper.year,\n",
    "    p.n_citation = paper.n_citation,\n",
    "    p.doi = paper.doi\n",
    "\n",
    "    // Match authors\n",
    "    WITH paper, p\n",
    "    UNWIND  paper.authors AS author\n",
    "    MERGE (a:Author {authorid: author.id})\n",
    "    ON CREATE SET a.name = author.name\n",
    "    MERGE (a)-[:AUTHORED]->(p)\n",
    "\n",
    "    // Match references\n",
    "    WITH paper, p\n",
    "    UNWIND  paper.references AS refid\n",
    "    MATCH (r:Paper {paperid:refid})\n",
    "    MERGE (p)-[:REFERENCES]->(r)\n",
    "    RETURN count(p:Paper) as total\n",
    "   '''\n",
    "\n",
    "   return insert_data(query, rows, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.query('CREATE INDEX paper_id_index IF NOT EXISTS FOR (n:Paper) ON (n.paperid);')\n",
    "conn.query('CREATE INDEX author_id_index IF NOT EXISTS FOR (a:Author) ON (a.authorid);')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/41 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     10\u001B[0m rows  \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m f:\n\u001B[1;32m---> 12\u001B[0m     rows\u001B[38;5;241m.\u001B[39mappend(\u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mline\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     13\u001B[0m     lines \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m lines \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m: \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ke-citation-graph\\lib\\json\\__init__.py:346\u001B[0m, in \u001B[0;36mloads\u001B[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[0;32m    341\u001B[0m     s \u001B[38;5;241m=\u001B[39m s\u001B[38;5;241m.\u001B[39mdecode(detect_encoding(s), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msurrogatepass\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[0;32m    344\u001B[0m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[0;32m    345\u001B[0m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n\u001B[1;32m--> 346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_decoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    348\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m JSONDecoder\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ke-citation-graph\\lib\\json\\decoder.py:337\u001B[0m, in \u001B[0;36mJSONDecoder.decode\u001B[1;34m(self, s, _w)\u001B[0m\n\u001B[0;32m    332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, s, _w\u001B[38;5;241m=\u001B[39mWHITESPACE\u001B[38;5;241m.\u001B[39mmatch):\n\u001B[0;32m    333\u001B[0m     \u001B[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001B[39;00m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;124;03m    containing a JSON document).\u001B[39;00m\n\u001B[0;32m    335\u001B[0m \n\u001B[0;32m    336\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 337\u001B[0m     obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_w\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    338\u001B[0m     end \u001B[38;5;241m=\u001B[39m _w(s, end)\u001B[38;5;241m.\u001B[39mend()\n\u001B[0;32m    339\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m end \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(s):\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "file = \"data/dblp_papers_v11.txt\"\n",
    "n_papers = 4107340 # Number of papers in the dataset\n",
    "subset = [\"id\", \"title\", \"year\", \"n_citation\", \"doi\", \"authors\", \"references\"]\n",
    "\n",
    "# TODO: Add tqdm max of the number of papers\n",
    "with open(file, 'r') as f:\n",
    "    for episode in tqdm(range(n_papers // 100000)):\n",
    "        try:\n",
    "            lines = 100000\n",
    "            rows  = []\n",
    "            for line in f:\n",
    "                rows.append(json.loads(line))\n",
    "                lines -= 1\n",
    "                if lines == 0: break\n",
    "            df = pd.DataFrame(rows)\n",
    "            add_papers(df[subset], 1000)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def process_stackexchange(board: str) -> list[pd.DataFrame]:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        board (str): Stackexchange board to load. Expected file structure is\n",
    "            ./data/`board`/*.xml\n",
    "\n",
    "    Returns:\n",
    "        list[pd.DataFrame]: List of pandas dataframes with Comments and\n",
    "            PostHistory data with extracted DOIs and markdown links.\n",
    "    \"\"\"\n",
    "    # Regex patterns\n",
    "    DOI_PATTERN = \"(10\\.\\d{4,9}/[-._;()/:A-Z0-9]+[/A-Z0-9])\"\n",
    "    MD_PATTERN = \"\\[([\\w\\s\\d]+)\\]\\((https?:\\/\\/[\\w\\d./?=#]+)\\)\"\n",
    "\n",
    "    def process_set(set: str) -> pd.DataFrame:\n",
    "        \"\"\"Processes single file in board data\n",
    "\n",
    "        Args:\n",
    "            set (str): One of \"Comments\" or \"PostHistory\"\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Original data with extracted DOIs and markdown links\n",
    "        \"\"\"\n",
    "        # Load xml file\n",
    "        path = f\"data/{board}/{set}.xml\"\n",
    "        with open(path, encoding=\"utf8\") as file:\n",
    "            df = pd.read_xml(file.read())\n",
    "\n",
    "        # Extract all DOIs using pattern defined before to list per row\n",
    "        df[\"DOI\"] = df[\"Text\"].apply(re.findall, args=(DOI_PATTERN,))\n",
    "\n",
    "        # Extract Markdown links\n",
    "        markdown_df = df[\"Text\"].apply(re.findall, args=(MD_PATTERN,))\n",
    "        markdown_df.columns = [\"LinkTitle\", \"LinkURL\"]\n",
    "\n",
    "        return pd.concat([df\n",
    "                             # , markdown_df\n",
    "                          ], axis=1)\n",
    "\n",
    "\n",
    "    return process_set(\"Comments\"), process_set(\"PostHistory\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "error",
     "evalue": "missing ), unterminated subpattern at position 188",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31merror\u001B[0m                                     Traceback (most recent call last)",
      "Input \u001B[1;32mIn [18]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m comments_df, posthistory_df \u001B[38;5;241m=\u001B[39m \u001B[43mprocess_stackexchange\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstats\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36mprocess_stackexchange\u001B[1;34m(board)\u001B[0m\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;66;03m# Extract Markdown links\u001B[39;00m\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;66;03m# markdown_df = df[\"Text\"].apply(re.findall, args=(MD_PATTERN,))\u001B[39;00m\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;66;03m# markdown_df.columns = [\"LinkTitle\", \"LinkURL\"]\u001B[39;00m\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mconcat([df\n\u001B[0;32m     41\u001B[0m                          \u001B[38;5;66;03m# , markdown_df\u001B[39;00m\n\u001B[0;32m     42\u001B[0m                       ], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 45\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mprocess_set\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mComments\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m, process_set(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPostHistory\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36mprocess_stackexchange.<locals>.process_set\u001B[1;34m(set)\u001B[0m\n\u001B[0;32m     31\u001B[0m     df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_xml(file\u001B[38;5;241m.\u001B[39mread())\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# Extract all DOIs using pattern defined before to list per row\u001B[39;00m\n\u001B[1;32m---> 34\u001B[0m df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDOI\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mText\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mre\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfindall\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mDOI_PATTERN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# Extract Markdown links\u001B[39;00m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;66;03m# markdown_df = df[\"Text\"].apply(re.findall, args=(MD_PATTERN,))\u001B[39;00m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;66;03m# markdown_df.columns = [\"LinkTitle\", \"LinkURL\"]\u001B[39;00m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mconcat([df\n\u001B[0;32m     41\u001B[0m                      \u001B[38;5;66;03m# , markdown_df\u001B[39;00m\n\u001B[0;32m     42\u001B[0m                   ], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32mC:\\Dev\\KE\\ke-citation-graph\\.venv\\lib\\site-packages\\pandas\\core\\series.py:4433\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[0;32m   4323\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4324\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4325\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4328\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4329\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4330\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4331\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4332\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4431\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4432\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 4433\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Dev\\KE\\ke-citation-graph\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1078\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m   1079\u001B[0m     \u001B[38;5;66;03m# if we are a string, try to dispatch\u001B[39;00m\n\u001B[0;32m   1080\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_str()\n\u001B[1;32m-> 1082\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Dev\\KE\\ke-citation-graph\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1131\u001B[0m         values \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m)\u001B[38;5;241m.\u001B[39m_values\n\u001B[0;32m   1132\u001B[0m         \u001B[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001B[39;00m\n\u001B[0;32m   1133\u001B[0m         \u001B[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001B[39;00m\n\u001B[0;32m   1134\u001B[0m         \u001B[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001B[39;00m\n\u001B[0;32m   1135\u001B[0m         \u001B[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001B[39;00m\n\u001B[0;32m   1136\u001B[0m         \u001B[38;5;66;03m# \"Callable[[Any], Any]\"\u001B[39;00m\n\u001B[1;32m-> 1137\u001B[0m         mapped \u001B[38;5;241m=\u001B[39m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1138\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1139\u001B[0m \u001B[43m            \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[0;32m   1140\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1141\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1144\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1145\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1146\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32mC:\\Dev\\KE\\ke-citation-graph\\.venv\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Dev\\KE\\ke-citation-graph\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:138\u001B[0m, in \u001B[0;36mApply.__init__.<locals>.f\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mf\u001B[39m(x):\n\u001B[1;32m--> 138\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(x, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ke-citation-graph\\lib\\re.py:240\u001B[0m, in \u001B[0;36mfindall\u001B[1;34m(pattern, string, flags)\u001B[0m\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfindall\u001B[39m(pattern, string, flags\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;124;03m\"\"\"Return a list of all non-overlapping matches in the string.\u001B[39;00m\n\u001B[0;32m    234\u001B[0m \n\u001B[0;32m    235\u001B[0m \u001B[38;5;124;03m    If one or more capturing groups are present in the pattern, return\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    238\u001B[0m \n\u001B[0;32m    239\u001B[0m \u001B[38;5;124;03m    Empty matches are included in the result.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 240\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_compile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mfindall(string)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ke-citation-graph\\lib\\re.py:303\u001B[0m, in \u001B[0;36m_compile\u001B[1;34m(pattern, flags)\u001B[0m\n\u001B[0;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m sre_compile\u001B[38;5;241m.\u001B[39misstring(pattern):\n\u001B[0;32m    302\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfirst argument must be string or compiled pattern\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 303\u001B[0m p \u001B[38;5;241m=\u001B[39m \u001B[43msre_compile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (flags \u001B[38;5;241m&\u001B[39m DEBUG):\n\u001B[0;32m    305\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(_cache) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m _MAXCACHE:\n\u001B[0;32m    306\u001B[0m         \u001B[38;5;66;03m# Drop the oldest item\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ke-citation-graph\\lib\\sre_compile.py:764\u001B[0m, in \u001B[0;36mcompile\u001B[1;34m(p, flags)\u001B[0m\n\u001B[0;32m    762\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m isstring(p):\n\u001B[0;32m    763\u001B[0m     pattern \u001B[38;5;241m=\u001B[39m p\n\u001B[1;32m--> 764\u001B[0m     p \u001B[38;5;241m=\u001B[39m \u001B[43msre_parse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse\u001B[49m\u001B[43m(\u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    765\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    766\u001B[0m     pattern \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ke-citation-graph\\lib\\sre_parse.py:948\u001B[0m, in \u001B[0;36mparse\u001B[1;34m(str, flags, state)\u001B[0m\n\u001B[0;32m    945\u001B[0m state\u001B[38;5;241m.\u001B[39mstr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m\n\u001B[0;32m    947\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 948\u001B[0m     p \u001B[38;5;241m=\u001B[39m \u001B[43m_parse_sub\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m&\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mSRE_FLAG_VERBOSE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    949\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Verbose:\n\u001B[0;32m    950\u001B[0m     \u001B[38;5;66;03m# the VERBOSE flag was switched on inside the pattern.  to be\u001B[39;00m\n\u001B[0;32m    951\u001B[0m     \u001B[38;5;66;03m# on the safe side, we'll parse the whole thing again...\u001B[39;00m\n\u001B[0;32m    952\u001B[0m     state \u001B[38;5;241m=\u001B[39m State()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ke-citation-graph\\lib\\sre_parse.py:443\u001B[0m, in \u001B[0;36m_parse_sub\u001B[1;34m(source, state, verbose, nested)\u001B[0m\n\u001B[0;32m    441\u001B[0m start \u001B[38;5;241m=\u001B[39m source\u001B[38;5;241m.\u001B[39mtell()\n\u001B[0;32m    442\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 443\u001B[0m     itemsappend(\u001B[43m_parse\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnested\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    444\u001B[0m \u001B[43m                       \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnested\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mand\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mitems\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    445\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m sourcematch(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m|\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    446\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ke-citation-graph\\lib\\sre_parse.py:836\u001B[0m, in \u001B[0;36m_parse\u001B[1;34m(source, state, verbose, nested, first)\u001B[0m\n\u001B[0;32m    834\u001B[0m p \u001B[38;5;241m=\u001B[39m _parse_sub(source, state, sub_verbose, nested \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    835\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m source\u001B[38;5;241m.\u001B[39mmatch(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 836\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m source\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmissing ), unterminated subpattern\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    837\u001B[0m                        source\u001B[38;5;241m.\u001B[39mtell() \u001B[38;5;241m-\u001B[39m start)\n\u001B[0;32m    838\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m group \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    839\u001B[0m     state\u001B[38;5;241m.\u001B[39mclosegroup(group, p)\n",
      "\u001B[1;31merror\u001B[0m: missing ), unterminated subpattern at position 188"
     ]
    }
   ],
   "source": [
    "comments_df, posthistory_df = process_stackexchange(\"stats\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "se_boards = [\"ai\", \"cstheory\", \"datascience\", \"softwarerecs\", \"stats\"]\n",
    "for board in se_boards:\n",
    "    comments_df, posthistory_df = process_stackexchange(\"stats\")\n",
    "    comments_df = comments_df[~comments_df[\"DOIs\"].isna()]\n",
    "    posthistory_df = posthistory_df.sort_values('CreationDate').drop_duplicates(subset=[\"PostId\", \"DOIs\"]).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3acfef7138f70d2429ae48cbaca4cc964d0577b5663fbd0bd04cb7bf77da6df6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('maat-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}